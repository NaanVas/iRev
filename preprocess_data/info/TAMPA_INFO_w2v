SAVE FOLDER： ../dataset/.data/Tamp_data_word2vec
Warning: the word embedding file is not provided, will be initialized randomly
2024-05-21 13:06:58: Step1: loading raw review datasets...
===============Start:all  rawData size======================
dataNum: 241257
userNum: 16528
itemNum: 8761
data densiy: 0.0017
===============End: rawData size========================
------------------------------------------------------------
2024-05-21 13:07:00 Step2: split datsets into train/val/test, save into npy data
===============Start: no-preprocess: trainData size======================
dataNum: 193005
userNum: 16525
itemNum: 8625
===============End: no-preprocess: trainData size========================
===============Start--process finished: trainData size======================
dataNum: 193208
userNum (config): 16528
itemNum (config): 8761
===============End-process finished: trainData size========================
2024-05-21 13:07:07
Train data size (config): 193208
Val data size (config): 24025
Test data size (config): 24025
------------------------------------------------------------
2024-05-21 13:07:07 Step3: Construct the vocab and user/item reviews from training set.
LDA transform matrix: (25289, 32)
The vocab size: 50002
Average user document length: 271.45607454017426
Average item document length: 286.54137655518775
2024-05-21 13:24:01
u_max_r:15
i_max_r:33
r_max_len：65
------------------------------------------------------------
2024-05-21 13:24:02 Step4: padding all the text and id lists and save into npy.
user document length: 500
item document length: 500
------------------------------------------------------------
2024-05-21 13:24:16 start writing npy...
2024-05-21 13:24:22 write finised
------------------------------------------------------------
2024-05-21 13:24:22 Step5: start word embedding mapping...
############################
out of vocab: 13548
w2v size: 50002
############################
Vocab Size and Word Dim: (50002, 300)
2024-05-21 13:25:12 all steps finised, cost time: 1093.7324s
